È naturale domandarsi riguardo la scelta di utilizzare algoritmi \emph{probabilistici} per cui c'è una possibilità di ottienere un risultato errato, rispetto agli algoritmi \emph{deterministici} in cui si ha la certezza di ottenere sempre un risultato corretto. In questa sezione si vedono degli algoritmi di esempio che giustificano questa scelta.
    \subsection{Verifica Identità polinomiali}
    \subsubsection{Algoritmo deterministico}
        Iniziamo descrivendo brevemente il problema, siano $f(x)$ e $g(x)$ due polinomi per cui $d = \deg(f) = \deg(g)$ per cui
        \[
            f(x) = \prod^d_{i=0}{(x-a_i)}
        \]
        mentre $g(x)$ è descritto nel modo seguente
        \[
            g(x) = \sum^{d}_{i=0}{(c_i \cdot x^i)}
        \]
        verificare l'identità $\forall x \ f(x) = g(x) $.
        
        Un'opzione è quella di convertire $f(x)$ nella forma canonica moltiplicando i fattori tra di loro, così facendo è sufficiente verificare che i coefficienti di ciascuna variabile siano uguali per i due polinomi ovvero per $a_i$ coefficienti di $f(x)$ e $c_i$ i coefficienti di $g(x)$ associati allo stesso $x^i$.
        $$ f(x) = g(x) \iff \forall i = 0,\dots,d \ a_i = c_i$$
        Questa soluzione è \textit{deterministica} e richiede $O(d^2)$ operazioni
        
    \subsubsection{Algoritmo probabilistico}
    Sia $d$ il grado dei due polinomi, come in precedenza. l'algoritmo sceglie u.a.r. un intero $r \in_u \{0,\dots,100d\}$ e calcola i valori di $f(r)$ e $g(r)$
    \begin{equation*}
        Output = 
        \begin{cases*}
            Vero \text{ se } f(r) = g(r)\\
            Falso \text{ altrimenti}
        \end{cases*}
    \end{equation*}
    Assumendo che la scelta di $r \in_u \{1,  \dots, 100d\}$ sia $O(1)$ e il calcolo di $f(r)$ e $g(r)$ sia $O(d)$, la complessità dell'algoritmo è $O(d)$.

    L'algoritmo probabilistico è molto migliore rispetto alla sua controparte deterministica, ma questo miglioramento comporta un costo non indifferente, l'algoritmo probabilistico in qualche caso potrebbe sbagliare.

    \vspace{2em}
    \emph{In quali casi l'algoritmo sbaglia?}
    \begin{example}
        \[
            F(x) = (x+2)(x+2)
            \quad
            G(x) = x^2+7x+1
        \]
        I due polinomi sono ovviamente diversi,
        per $r = 2$ si ha $F(2) = 19$ e $G(2) = 16$, mentre per $r=1$ $F(1) = 9 = G(1)$ 
        ne segue che se l'algoritmo secgliesse $r = 2$ si arriverebbe a una risposta errata.
    \end{example}
    Sia $f(x) \neq g(x)$, e la somma dei gradi di $x$ in $f$ e $g$ è limitato da $d$. Definiamo $h(x)$ come segue:
    \[
        h(x):= f(x) - g(x)
    \]
    naturalmente anche $h(x)$ è limitato nel grado da $d$, e l'equazione $h(x) = 0$ ha al massimo $d$ soluzioni. È banale convincersi che il caso $h(x) = 0$ descrive esattamente i casi in cui l'algoritmo probabilistico sbaglia.

    \subsubsection*{Analisi dell'algoritmo}
    \begin{enumerate}
        \item Se l'identità $f(x) \equiv g(x)$ è vera, l'algoritmo da \emph{sempre} in output la risposta corretta
        \item Se l'identità $f(x) \equiv g(x)$ non è vera, l'algoritmo sbaglia in \emph{alcuni} casi
    \end{enumerate}
    Si ha che scegliendo $r \in_u [1,100d]$ allora \fbox{$\Pr[f(r) = g(r)] < \frac{1}{100}$} \label{ris:1}(la probabilità che l'algoritmo sbagli è bassa).
    
    \vspace{1em}
    Si generalizza l'algoritmo per $k$ prove indipendenti
    \begin{itemize}
        \item Evento semplice $E_s$: fissata una sequenza specifica $r_1,\dots,r_k \in_u [1,100d]$, l'evento semplice rappresenta la scelta dell'algoritmo di quella specifica sequenza
        \item  Evento negativo $E_n$: la sequenza scelta contiene tutte radici di $h(x)$
        \item Se in qualsiasi delle $k$ prove l'output è $Falso$ restituisci $Falso$
    \end{itemize}
    Si calcolano la probabilità di $E_s$ ed $E_n$
    \[
        \Pr{[E_s]} = \left({\frac{1}{100d}}\right)^k
        \quad
        \Pr{[E_n]} \leq d^k \cdot \left( \frac{1}{100d} \right)^k = { \left( \frac{1}{100} \right) }^k
    \]
    Ci sono al massimo $d^k$ sequenze in cui tutte le $r_i$ scelte sono radici di $h(x)$, inoltre da un risultato precedente (\ref{ris:1}), la probabilità che una sequenza contenga una radice è $\leq \frac{1}{100}$. Poiché le scelte sono indipendenti, la probabilità di $k$ scelte successive errate è proprio ${ \left( \frac{1}{100} \right) }^k$.
    
    \vspace{1em}
    \noindent
    L'evento $E_n$ rappresenta quindi l'evento in cui l'algoritmo da un risultato errato, questo avviene con probabilità $\leq { \left( \frac{1}{100} \right) }^k$.

    \subsection{Verifica Moltiplicazione matriciale}
    Siano $A,B,C \in \{0,1\}^{n\times n}$ tre matrici binarie, verificare 
    \[
        A\cdot B = C
    \]
    \subsubsection{Algoritmo deterministico}
    L'algoritmo \emph{deterministico} richiede l'uso della moltiplicazione tra matrici,si ricorda la moltiplicazione tra matrici, sia $AB = A \cdot B$
    \[
        AB = 
        \begin{bmatrix}
            c_{11} & \cdots & c_{1n} \\ 
            \vdots & \ddots & \vdots \\
            c_{n1} & \cdots & c_{nn}
        \end{bmatrix}
    \]
    In cui ogni termine $c_{ij} = \sum_{k=1}^{n}a_{ik}b_{kj}$. Un algoritmo banale impiega $\Theta(n^3)$ tempo con un metodo standard, o $\Theta(n^{2.37})$ con un metodo più efficiente. 
    
    \subsubsection{Algoritmo probabilistico}
    Iniziamo subito definendo il comportamento di un algoritmo randomizzato, nettamente più efficiente dell'algoritmo deterministico visto in precedenza
    \begin{enumerate}
        \item Scelta di un vettore $\bar{r} = (r_1,\dots,r_n) \in_u \{0,1\}^n$
        \item Calcolo di $B\cdot\bar{r}$
        \item Calcolo $A(B\cdot \bar{r})$
        \item Calcolo $C\cdot\bar{r}$
    \end{enumerate}
    \[
    Output = 
    \begin{cases*}
        Vero \text{ se } A(B\cdot \bar{r}) = C\cdot\bar{r}\\
        Falso \text{ altrimenti }    
    \end{cases*}
    \]
    L'algorimo esegue moltiplicazioni tra le matrici e il vettore scelto $\bar{r}$, più efficienti rispetto alla moltiplicazione tra matrici, di seguito un analisi per determinare l'efficacia in termini probabilistici dell'algoritmo.

    \subsubsection*{Analisi}
    Il seguente risultato fornisce una base utile per determinare la probabilità di successo dell'algoritmo
    \begin{theorem}
        Se $AB \neq C$ e $r$ è scelto uniformemente da $\{0,1\}$ allora 
        \[
            \Pr{[AB\cdot\bar{r} = C\cdot\bar{r}]} \leq \frac{1}{2}
        \]
    \end{theorem}
    Si ricorda che scegliere $\bar{r} \in_u \{0,1\}^n$ è equivalente a scegliere $n$ valori indipendenti $r_1,\dots,r_n \in_u \{0,1\}$  
    \begin{proof}
        Sia $D=AB - C \neq 0$ una matrice in $\{0,1\}^{n\times n}$ diversa da 0, ovvero $\exists d_{ij} \neq 0$
        \[
            AB\cdot\bar{r} = C\cdot\bar{r} \implies D\cdot\bar{r} = 0
        \]
        Per semplicità assumiamo che $d_{ij} \neq 0$ sia proprio $d_{11}$. Poiché $D\cdot\bar{r} = 0$ 
        \[
            \sum_{j=1}^nd_{1j}r_j = 0 \quad \text{ ovvero } \quad r_1 = -\frac{\sum_{j=2}^n{d_{1j}r_j}}{d_{11}}
        \]
        Assumiamo di aver fissato una sequenza $r_2,\dots,r_n$ allora è possibile determinare facilmente $r_1$. La probabilità che $r_1$ rispetti l'uguaglianza è $\leq \frac{1}{2}$

        \vspace{1em}

        Sia $X = (x_2,\dots,x_n) \in \{0,1\}^{n-1} = B^{n-1}$
        \begin{align*}
            \Pr{[AB\cdot\bar{r} = C\cdot\bar{r}]} &= \sum_{ x \in B^{n-1}}\Pr{[AB\cdot\bar{r} = C\cdot\bar{r} \mid(r_2,\dots,r_n) = (x_2,\dots,x_n)]} \cdot \Pr{[(r_2,\dots,r_n) = (x_2,\dots,x_n)]} =\\
            &= \sum_{x \in B^{n-1}}\Pr{[ (AB\cdot\bar{r} = C\cdot\bar{r}) \cap (r_2,\dots,r_n) = (x_2,\dots,x_n)]}\\
            &\leq \sum_{x \in B^{n-1}}\Pr{\left[\left(r_1 = -\frac{\sum_{j=2}^n{d_{1j}r_j}}{d_{11}}\right) \cap (r_2,\dots,r_n) = (x_2,\dots,x_n)\right]} =\\
            &= \sum_{x \in B^{n-1}}\Pr{\left[r_1 = -\frac{\sum_{j=2}^n{d_{1j}r_j}}{d_{11}}\right]} \cdot \Pr{[(r_2,\dots,r_n) = (x_2,\dots,x_n)]}\\
            &\leq \sum_{x \in B^{n-1}}\frac{1}{2}\Pr{[(r_2,\dots,r_n) = (x_2,\dots,x_n)]}\\
            &= \frac{1}{2}.
        \end{align*}
    \end{proof}
    Anche in questo caso è possibile ripetere l'algoritmo su una sequenza di $k$ prove, si ha interesse a capire come cambia la confidenza sull'esito. Non avendo informazioni sulla provenienza di $A,B,C$ è ragionevole assumere che $A\cdot B = C$ sia vero con probabilità $\frac{1}{2}$. A questo scopo si definiscono gli eventi $E$ che definisce la correttezza dell'identità, $B$ l'evento in cui l'algoritmo restituisce $Vero$.
    
    \vspace{1em}
    Si inzia con $\Pr(E_0) = \Pr(\bar{E_0}) = \frac{1}{2}$ e poiché l'algorimo è a \emph{one-sided-error} $\Pr(B\mid E) = 1$, e $\Pr(B \mid \bar{E}) \leq \frac{1}{2}$. Ovvero è banale convincersi che la probabilità di successo dell'algorimo, data un'identità falsa, è certo. Applicando Bayes si può ottenere una formula per determinare la confidenza della correttezza alla prova $i+1$ sapendo la confidenza e il risultato della prova \textit{i-esima}
    \[
        \Pr{[E_{i+1}]} =  \frac{\Pr(B_i\mid E_i)\Pr{[E_i]}}{\Pr(B_i\mid E_i)\Pr{[E_i]} + \Pr(B_i\mid \bar{E_i})\Pr{[\bar{E_i}]}}
    \]
    In generale se prima di eseguire l'algoritmo l'\textit{i-esima} volta $\Pr{[E_i]} \geq \frac{2^i}{(2^i+1)}$ e l'algorimo restituisce $Vero$ ($\Pr{[B_i]}=1$) allora
    % TODO: Mostrare meglio i passaggi magari con un ref ad un'altra pagina
    \[
        \Pr{[E_{i+1} \mid B_i]} \geq \frac{\frac{2^i}{(2^i+1)}}{\frac{2^i}{(2^i+1)} + \frac{1}{2}\frac{1}{(2^i+1)}} = \frac{2^{i+1}}{2^{i+1}+1} = 1 - \frac{1}{2^i+1}
    \]
    La confidenza alla \textit{i-esima} prova è $\geq 1 - \frac{1}{2^i+1}$. Con questo risultato si può affermare che la confidenza nella corretta del risultato dell'algorimo cresce esponenzialmente nel numero di prove effettuate.
    
    \subsection{Problema Min-Cut}
    Spostiamo l'attenzione ora su un problema noto, il problema del \emph{Min-Cut}. Sia $G = \langle V,E \rangle$ un grafo. Un taglio (cut) è una partizione dei vertici $V$ in due sottoinsiemi non vuoti $A,B \subset V$ tali che $B = V-A$. L'insieme degli archi del taglio è definito come
    \[
        C = \{(u,v) \in E \mid u \in A, v \in B\}
    \]
    Il problema del \emph{min-cut} consiste nel trovare $C$ per cui $|C|$ è minima. $C$ inoltre è il minimo insieme di archi per cui il grafo $G$ è sconnesso.

    \subsubsection{Algoritmo probabilistico}
    L'algoritmo probabilistico per risolvere il Min-Cut è basato sull'operazione di \emph{contrazione} dei nodi. Il processo di contrazione avviene nel seguente modo
    \begin{enumerate}
        \item Si seleziona un arco $e=(u,v) \in_u E$ (in modo uniforme)
        \item Si fondono i nodi $u$ e $v$ in un unico nodo.
        \item Si rimuovono i \emph{self-loops} risultanti dalla contrazione.
    \end{enumerate}
    \input{figure/grafo.tex}
    Si descrive ora l'algorimo probabilistico 
    \begin{enumerate}
        \item Ripeti $n-2$ volte:
            \begin{enumerate}
                \item Prendi $(u,v) \in_u E$
                \item Contrai $u,v$, ed elimina i \emph{self-loops}
            \end{enumerate}
        \item Restituisci l'insieme di archi che connettono i due vertici rimanenti
    \end{enumerate}

    \begin{theorem}
        L'algoritmo restituisce un \emph{min-cut} con probabilità $\geq \frac{1}{n(n-1)}$
    \end{theorem}

    \begin{lemma}
        La contrazione dei vertici non riduce la grandezza del \emph{min-cut}, può solo aumentare
    \end{lemma}
    \begin{proof}
        Ogni \emph{cutset} nel nuovo multigrafo è un \emph{cutset} nel grafo prima della contrazione
    \end{proof}

    \subsubsection*{Analisi dell'algorimo}
    Per analizzare la probabilità di successo dell'algorimo, si parte supponendo che il grafo abbia un min-cut $C$ di $k$ archi, si calcola ora la probabilità di trovare tale \emph{cutset} $C$.

    \begin{lemma}
        Se l'arco contratto non appartiene al min-cut $C$, allora nessun altro arco eliminato appartiene a $C$.
    \end{lemma}
    \begin{proof}
        Il processo di contrazione di $u$ e $v$ elimina gli archi paralleli $e_1 (u,v) \in E$, $e_2 (u,v) \in E$ ovvero tutti gli archi con estremi $u,v$. Tali archi appartengono o meno, contemporaneamente a $C$.
    \end{proof} 
    Si definiscono quindi due eventi $E_i$ che descrive l'evento in cui l'arco contratto all'iterazione \emph{i-esima} non appartiene a $C$, $F_i$ l'evento in cui nessun arco di $C$ è stato contratto nelle prime $i$ iterazioni $F_i = \bigcap_{j=1}^iE_j$. È facile vedere che $F_{n-2}$ è proprio l'evento in cui l'algorimo restituisce l'insieme corretto.

    \vspace{1em}\noindent
    Si parte notando che se $|C| = k$ allora la cardinalità di tutti i nodi è $\geq k$, inoltre il grafo ha almeno $\frac{nk}{2}$ archi. Per cui selezionando un arco $e \in E$ casualmente (in modo uniforme), la probabilità che $e$ sia un arco di $C$ è $\Pr(e\in E) \leq \frac{k}{\frac{nk}{2}} = \frac{2}{n}$, questo caso è proprio l'evento complementare di $E_1$, per cui $\Pr(E_1) \geq 1-\frac{2}{n}$. Ora assumendo che $E_1$ si sia verificato, rimangono $n-1$ nodi con un \emph{min-cut} di dimensione e grado minimo $\geq k$. Supponendo che in $i-1$ iterazioni l'algorimo non abbia mai selezionato nessun arco in $C$ ovvero verificando l'evento $F_{i-1}$ si deriva la formula
    \[
        \Pr{[E_i \mid F_{i-1}]} \geq 1 - \frac{k}{\frac{k(n-i+1)}{2}} = 1 - \frac{2}{n-i+1}
    \]
    
    Per determinare la probabilità di successo dell'algorimo si deve calcolare $\Pr{[F_{n-2}]}$.
    \begin{align*}
        \Pr{[F_{n-2}]} &= \Pr{[E_{n-2}\cap F_{n-3}]} = \Pr{[E_{n-2} \mid F_{n-3}]}\Pr{[F_{n-3}]} = \\
        &= \Pr{[E_{n-3} \mid F_{n-4}]}\cdots\Pr{[E_2\mid F_1]}\Pr{[F_1]} \geq\\
        &\geq \prod_{i=1}^{n-2}\left( 1 - \frac{2}{n-i+1} \right) = \prod_{i=1}^{n-2} \frac{n-1-1}{n-i+1} =\\
        &= \left( \frac{n-2}{n}\right) \left( \frac{n-3}{n-1}\right)\left( \frac{n-4}{n-2} \right) \cdots \left( \frac{2}{4}\right)\left(\frac{1}{3}\right) = \frac{2}{n(n-1)}
    \end{align*}
    Come al solito si analizza l'aumento della confidenza all'aumentare del numero di esecuzioni dell'algoritmo. Le diverse esecuzioni sono indipendenti per cui per $k$ prove si ha la probabilità di non trovare un \emph{min-cut} $ \leq \left( 1- \frac{2}{n(n-1)}\right)^k$. Per $k = n(n-1)\log n$ 
    si ha $ \left( 1- \frac{2}{n(n-1)}\right)^k \leq e^{-2\log n} = \frac{1}{n^2}$.
    \subsection{Quick Sort}
    Si passa ora al caso di un'altro algoritmo noto, il \emph{QuickSort}, un algoritmo di ordinamento che dato $S$ in input restituisce $S$ ordinato in output.
    \subsubsection{Algoritmo deterministico}
    Si descrivono i passaggi dell'algoritmo \emph{QuickSort} deterministico
    \begin{enumerate}
        \item Prendi il perno $y$ il primo elemento in $S$
        \item Confronta tutti gli elementi di $S$ con $y$, a tale scopo si definiscono due insiemi \[
            S_1 = \{ x \in S - \{y\} \mid x \leq y\} \quad S_2 = \{ x \in S-\{y\} \mid x > y\}
        \]
        \item Restituisci la lista $Det\_QS(S_1),y,Det\_QS(S_2)$
    \end{enumerate}
    \subsubsection{Algoritmo probabilistico}
    Qui di seguito sono riportati brevemente i passaggi dell'algoritmo
    \begin{enumerate}
        \item Scegli un elemento per il perno $y \in_u S$
        \item Confronta tutti gli elementi di $S$ con $y$, a tale scopo si definiscono due insiemi \[
            S_1 = \{ x \in S - \{y\} \mid x \leq y\} \quad S_2 = \{ x \in S-\{y\} \mid x > y\}
        \]
        \item Restituisci la lista $QS(S_1),y,QS(S_2)$
    \end{enumerate}
    \subsubsection*{Analisi degli algoritmi}
    In questo caso iniziamo analizzando l'algoritmo probabilistico. Siano $s_1,\dots,s_n$ gli elementi in $S$ ordinati, per determinare il numero di confronti eseguiti si definisce la variabile aleatoria binaria $X_{ij}$
    \[
        X_{ij} = \begin{cases*}
            1 \text{ se } s_i \text{ è confrontato con } s_j\\
            0 \text{ altrimenti }
        \end{cases*}
    \]
    Sia allora $T$ la variabile che conta il numero di confronti totali durante l'esecuzione dell'algoritmo
    \[
        T = \sum_{i=0}^n\sum_{j>i}X_{ij}
    \]
    poiché si sta considerando di un algorimo probabilistico si è interessati a calcolare il valore atteso di $T$, $\E{T}$. Si vuole calcolare $\Pr[X_{ij} = 1]$, $s_i$ è confrontato con $s_j$ solo nel caso in cui uno dei due elementi è scelto come \emph{perno} prima che gli $j-i-1$ elementi tra $s_i$ e $s_j$ siano scelti. Il perno è scelto in modo uniforme, per cui anche gli elementi $[s_i,\dots,s_j]$ sono scelti in modo uniforme, si ottienere
    \begin{align*}
        &\Pr{[X_{ij} = 1]} = \frac{2}{j-i+1}\\
        &\E{X_{ij}} = \frac{2}{j-i+1}
    \end{align*}
    Facendo uso delle proprietà del valore atteso
    \begin{align*}
        \E{T} &= \E{\sum_{i=0}^n\sum_{j>i}X_{ij}} = \sum_{i=0}^n\sum_{j>i}\E{X_{ij}}=\\
        &= \sum_{i=0}^n\sum_{j>i}\frac{2}{j-i+1} \leq\\
        &\leq n \underbrace{\sum_{k=1}^{n} \frac{2}{k}}_{
            \substack{\text{numero}\\ \text{armonico}}
        } = 2n \log n + O(n)
    \end{align*}
    Per cui il numero atteso di confronti nell'esecuzione del \emph{QuickSort} probabilistico è $O(n\log n)$.
    
    \vspace{1em}\noindent
    Si passa ora all'analisi dell'algoritmo deterministico, ponendo come premessa un input $S$ perfettamente scelto da tutte le permutazioni di elementi. Ponendo questa premessa, se tutte le permutazioni hanno la stessa probabilità (quindi uniforme) allora anche il sottoinsieme $[s_i,\dots,s_j]$ ha probabilità uniforme.

    L'analisi dell'algoritmo in questo particolare caso è analoga alla versione probabilistica, si ottiene tempo di esecuzione $O(n \log n)$. È importante notare che nel caso deterministico è necessario che l'input sia distribuito in modo uniforme, ciò è un'assunzione molto forte.
    
    \subsection{Algoritmo per il Calcolo della Mediana}
    %TODO: Inserire sezione richiami di probabilità con esempi (cupons collector)
    Come visto in \textcolor{red}{Sezione 0} la mediana è un indice statistico. Dato $X$ un insieme non ordinato di elementi restituirene la mediana. Un algorimo deterministico banale consiste nell'ordinare $X$ e restituire l'elemento corretto, per una complessittà di $O(n\log n)$. Esiste anche un algoritmo deterministico che opera in $O(n)$.

    \subsubsection{Algoritmo probabilistico}
    L'algoritmo probabilistico prende in input $S$ un insieme di $2k +1$ elementi da un'universo totalmente ordinato e restituisce in outupt \emph{$k+1$-esimo} elemento nell'insieme.
    
    L'algoritmo opera come segue
    \begin{enumerate}
        \item Scegli un \emph{multi-insieme} $R$ di $n^{3/4}$ elementi presi da $S$, Ordina $R$
        \item Sia $d$ l'elemento $\left(\frac{1}{2}n^{3/4} - \sqrt{n}\right)$\emph{-esimo} più piccolo in $R$ ordinato
        \item Sia $u$ l'elemento $\left(\frac{1}{2}n^{3/4} + \sqrt{n}\right)$\emph{-esimo} più piccolo in $R$ ordinato
        \item Calcola l'insieme $C$ di elementi compresi tra $d$ e $u$, inoltre si definiscono $\ell_d,\ell_u$
        \[
            C =\{ x \in S \mid d \leq x \leq u \} \quad \ell_d = |\{ x \in S \mid x < d\}| \quad \ell_u = |\{ x \in S \mid x > u\}|
        \]
        \item Se $\ell_d > \frac{n}{2}$ o $\ell_u > \frac{n}{2}$ termina l'esecuzione con errore
        \item Se $|C| < 4n^{3/4}$ ordina $C$
        \item Restituisci in output l'elemento $\left( \lfloor \frac{n}{2}\rfloor - \ell_d +1\right)$\emph{-esimo} in $C$ ordinato.
    \end{enumerate}
    Facciamo chiarezza sull'algoritmo spiegando più nel dettaglio i diversi punti. Partiamo con la costruzione di $R$ da $S$.
    \input{algorithms/median}
    La probabilità che un elemento $u$ non finisca in $R$ in una estrazione è $1 - \frac{1}{n}$, per $k$ estrazioni indipendenti si ha $\left( 1- \frac{1}{n}\right)^{k}$ ovvero la probabilità di non inserire $u$ in $k$ estrazioni successive. Per cui la probabilità che un qualsiasi elemento $u$ di $S$ sia finito in $R$ è $1 - \left( 1- \frac{1}{n}\right)^{n^{3/4}}$.
    \[
        \forall u \in S \ \Pr{[u \in R]} = 1 - \left( 1- \frac{1}{n}\right)^{n^{3/4}}\approx 1 - e^{-\frac{1}{n^{3/4}}} \approx \frac{1}{n^{1/4}}
    \]
    Quindi si ottiene un \emph{multi-insieme} $R$ che rappresenta un ``campione'' uniforme di $S$, e due elementi $d,u$ in cui sono inclusi circa $2\sqrt{n}$ valori intermedi, per cui gli elementi in $R$ sono distribuiti circa uniformemente tra gli elementi di $S$.
    Si ha l'insieme $C$ per cui $|C| < \frac{n}{\log n}$, per cui è possibile ordinare $C$ in tempo lineare.
    \input{figure/figura1.tex}

    Con alta probabilità $\frac{1}{2}n^{3/4} - \sqrt{n}$ elementi sono minori della mediana, mentre $\frac{1}{2}n^{3/4} - \sqrt{n}$ elementi sono maggiori della mediana, dato questo, allora la mediana appartiene al sottoinsieme $C$, se queste condizioni sono vere la mediana si trova proprio in posizione $\lfloor \frac{n}{2} \rfloor - \ell_d + 1$ di $C$ ordinato. 

    \vspace{1em}\noindent
    L'algoritmo come visto sopra ha anche dei casi di fallimento, si dà una forma più rigorosa agli eventi che portano l'algoritmo al fallimento. Si definiscono le variabili aleatorie $Y_1,Y_2$. $Y_1$ conta il numero di elementi in $R$ (campioni) soto la mediana, al contrario $Y_2$ conta il numero di elementi sopra la mediana. Formalmente si definisce $y_{1}^i$ la variabile binaria 
    \[
        y_{1}^i = \begin{cases*}
            1 \text{ se } x_i \in R < Mediana\\
            0 \text{ altrimenti}
        \end{cases*}
        \quad
        Y_1 = \sum_{i = 1}^{n^{3/4}}y_{1}^i
    \]  
    Si definiscono tre eventi $E_1$ per cui $C$ (l'intervallo di elementi $[d,u]$) è spostato a eccessivamente sinistra, l'evento $E_2$ è simmetrico, l'evento $E_3$ rappresenta una scelta di elementi in $R$ non sufficientemente uniformeme. 
    \begin{align*}
        &E_1: Y_1 < \frac{1}{2}n^{3/4}-\sqrt{n}\\
        &E_2: Y_2 < \frac{1}{2}n^{3/4}-\sqrt{n}\\
        &E_3: |C| > \frac{n}{\log n}
    \end{align*}
    È necessario uno solo di questi eventi per il fallimento dell'algoritmo, si calcolano le probabilità di avvenimento. Lo spazio di campionamento e l'insieme di tutte le possibili scelte di $n^{3/4}$ elementi da $S$ con ripetizione, ovvero $n^{n^{3/4}}$ punti. Per calcolare la probabilità dell'evento $E_1$ (legato alla variabile aleatoria $Y_1$),
    \[
        \E{y_1^i} = \frac{1}{2}
        \quad
        \Var{[y_1^i]} = \frac{1}{4}
    \]
    Inoltre 
    \begin{align*}
        &\Pr{[y_1^i = 1] = \frac{1}{2}}\\
        &\E{Y_1} = \E{\sum_{i=0}^{n^{3/4}}{y_1^i}} = \sum_{i=0}^{n^{3/4}}{\E{y_1^i}} = \frac{1}{2}n^{3/4}
    \end{align*}
    Si ottiene un risultato simile per la varianza $\Var{[y_1^i]} = \frac{1}{4}$. 

    \vspace{1em}\noindent
    Per la disuguaglianza di Chebyshev si ha
    \begin{align*}
        \Pr{[E_1]} &= \Pr{\left[Y_1 < \frac{1}{2}n^{3/4} - \sqrt{n} \right]} \leq\\
        &\leq \Pr{\left[|Y_1 - \E{Y_1}| > \sqrt{n}\right]} \leq\\
        &\leq \frac{\Var{[Y_1]}}{\left(\sqrt{n}\right)^2} = \boxed{\frac{1}{4n^{1/4}}}
    \end{align*}
    Per l'evento $E_2$ si ottiene un risultato simile $\Pr{[E_2]} \leq \frac{1}{4n^{1/4}}$, infine con alta probabilità $\Pr{[E_1 \cup E_2]} \leq \boxed{\frac{2}{4n^{1/4}}}$.

    \vspace{1em}\noindent
    Così facendo si è posto un \emph{upper-bound} alla probabilità del verificarsi dei primi due eventi $E_1,E_2$, per calcolare la probabilità di $E_3$ si definiscono altri due eventi $\mathcal{E}_{3,1}$ $\mathcal{E}_{3,2}$ che si verificano rispettivamente quando almeno $2n^{3/4}$ elementi in $C$ sono più grandi e più piccoli della mediana.
    \begin{align}
        \mathcal{E}_{3,1} = |\{ x \in C \mid x > M \}| > 2n^{3/4} \quad
        \mathcal{E}_{3,2} = |\{ x \in C \mid x < M \}| > 2n^{3/4}
        \label{event:1}
    \end{align}
    Si dimostra facilmente che per $|C| > 4n^{3/4}$ almeno uno dei due eventi è verificato, poiché si assume che la mediana $M$ sia in $C$. Prendiamo in considerazione l'evento $\mathcal{E}_{3,2}$, si vuole calcolare il numero di elementi in $R$ fuori da $C$, ricordando che $|R| = n^{3/4}$
    \[
        |\{ x \in R \mid x > u\}| = n^{3/4} - \left(\frac{1}{2}n^{3/4}+\sqrt{n}\right) = \frac{n^{3/4}}{2} - \sqrt{n}
    \]
    Inoltre dalla definizione di $\mathcal{E}_{3,2}$ (\ref{event:1}) allora $u$ è almeno l'elemento \emph{$\frac{1}{2}n+2n^{3/4}$-esimo} più grande in $S$ ($2n^{3/4}$ sono gli elementi più grandi della mediana $M$). Per cui almeno $\frac{1}{2}n^{3/4} - \sqrt{n}$ campioni di $R$ sono stati selezionati tra i $\frac{1}{2}n - 2n^{3/4}$ più grandi in $S$. 

    \vspace{1em}
    Sia ora $X$ la variabile aleatoria che conta il numero di campioni tra gli $\frac{1}{2}n - 2n^{3/4}$ elementi più grandi in $S$
    \begin{gather}
        X_i = \begin{cases*}
            1 \text{ l'elemento \emph{i-esimo} campiona tra i più grandi}\\
            0 \text{ altrimenti}
        \end{cases*}\\
        X = \sum_{i=0}^{n^{3/4}}X_i
    \end{gather}
    Quindi banalmente per $1/2n - 2n^{3/4}$ numero di elementi favorevoli, su $n$ numero di elementi totali
    \[
        \Pr{[X_i = 1]} = \frac{1/2n - 2n^{3/4}}{n}
    \]
    Inoltre si calcolano i valori attesi e varianze
    \begin{gather*}
        \E{X_i} = \E{(X_i)^2} = \frac{1}{2}- \frac{2}{n^{1/4}}\\
        \Var{[X_i]} = \E{(X_i)^2} - \E{X_i}^2 \leq \frac{1}{4}\\
        \E{X} = n^{3/4} \left( \frac{1}{2}- \frac{2}{n^{1/4}}\right)=\frac{1}{2}n^{3/4} - 2\sqrt{n}\\
        \Var{[X]}\leq \frac{1}{4}n^{3/4}
    \end{gather*}
    Facendo uso della disuguaglianza di Chebyshev si da un \emph{bound} alla probabilità dell'evento $\mathcal{E}_{3,1}$
    \begin{align*}
        \Pr{[\mathcal{E}_{3,1}]} &= \Pr{[X \geq \frac{1}{2}n^{3/4} - \sqrt{n}]}\\
        &\leq \Pr(|X- \E{X}| \geq \sqrt{n})\\
        &\leq \frac{\Var[X]}{n} = \frac{\frac{n^{3/4}}{4}}{n} = \frac{1}{4n^{1/4}}.
    \end{align*}
    In modo del tutto simile $\Pr{\left[\mathcal{E}_{3,2}\right]}\leq \frac{1}{4n^{1/4}}$ tornando alla probabilità dell'evento principale $E_3$
    \[
        \Pr{[E_3]} \leq \Pr{[\mathcal{E}_{3,2}]} + \Pr{[\mathcal{E}_{3,2}]} = \boxed{\frac{1}{2n^{1/4}}}.
    \]
    Concludendo la probabilità fallimento e successo dell'algoritmo sono
    \begin{align*}
        \Pr{[\text{Fallimento}]} &= \Pr{[E_1]} + \Pr{[E_2]} + \Pr{[E_3]} \\
        &\leq \frac{2}{4n^{1/4}} + \frac{1}{2n^{1/4}} = \frac{1}{n^{1/4}}\\
        \Pr{[\text{Successo}]} &= 1 - \Pr{[\text{Fallimento}]} \geq 1 - \frac{1}{n^{1/4}}.
    \end{align*}