In questa sezione si parla di problemi per cui la dimensione dei dati è eccessiva, ad esempio si potrebbe essere interessati a valutare uno dei seguenti problemi
\begin{enumerate}
    \item \textit{Pagine con una grande porzione di parole simili}
    \item \textit{Clienti con acquisti simili}
    \item \textit{Immagini con elementi silimi}
\end{enumerate}
\begin{example}
    Input: punti ad alte dimensioni $x_1,x_2,\dots,x_N$
    \[
        \begin{bmatrix}
            1 & 4 & 1\\
            0 & 2 & 1\\
            0 & 1 & 0\\
        \end{bmatrix}
        \implies
        \begin{bmatrix}
            1 & 4 & 1 & 0 & 2 & 1 & 0 & 1 & 0
        \end{bmatrix} \in \{ 0,1,\dots,c\}^h.
    \] per $h$ molto grande
    Si introduce una funzione di distanza $d(x_1,x_2)$ che quantifichi quanto i due dati sono \emph{vicini}.
    Un'applicazione comune consiste nel trovare tutte le coppie di dati in un insieme di dati che sono ``abbastanza'' vicini (rispetto a una soglia $s$) per cui $d(x_1,x_2) \leq s$.

    Una soluzione deterministica banale richiede tempo $O(N^2\cdot h)$ con $N$ numero di punti e $h$ dimensione dei dati. Banalmente si calcola la distanza per ogni coppia possibile $\binom{n}{2} = O (n^2)$.
\end{example}

È possibile trovare coppie di dati simili in tempo $O(N \cdot h')$ con $h' << h$, come spiegato di seguito.
\subsection{Document Similarity}
Dato un grande insieme di documenti $U^*$ grande $N \approx 10^9$, si vogliono trovare i documenti simili. Le applicazioni possibili riguardano la ricerca di siti web duplicati e la ricerca di notizie simili.

Il problema descritto presenta diverse sfide 
\begin{itemize}
    \item molti pezzi piccoli di un documento possono essere in ordine diverso
    \item il numero di documenti ($N$) è troppo elevato per confrontare tutte le coppie
    \item i documenti possono essere talmente grandi ($h$) da non poter essere mantenuti in memoria 
\end{itemize}

È inoltre necessario definire una funzione di distanza adeguata, a tale scopo si fa uso della \emph{Jaccard Similarity} definita su due insiemi $C_1,C_2$ come segue
\[
    \text{Jaccard similarity}(C_1,C_2) = \frac{|C_1 \cap C_2|}{|C_1 \cup C_2|}
\]
Si definisce anche un'altra misura \emph{Jaccard Distance} $d(C_1,C_2) = 1- \jsim(C_1,C_2)$. Resta ora da definire come applicare la Jaccard Similarity a due documenti (stringhe di lunghezza finita). 

Si definiscono i punti chiave della risoluzione del problema
\begin{enumerate}
    \item Input: Un universo $U$ enorme di documenti
    \item Shingling: conversione dei documenti in \emph{grandi insiemi}
    \item Min-Hashing: conversione di grandi insiemi in \emph{piccole firme} mantenendo la $\jsim$
    \item Local-Sensitive-Hashing: Individuare le firme che potrebbero essere simili 
    \item Output: coppie di documenti candidati 
\end{enumerate}

Restano ora da definire nel dettaglio le tecniche descritte
\subsubsection{Shingling}
Facciamo uso della tecnica di \emph{Shingling} per convertire i documenti in insiemi. Un \emph{k-shingle} per un documento è una sequenza di $k$ token che appaiono in un documento. I \emph{token} possono generalmente essere definiti come: caratteri o parole, si fa uso di $U_t$ per rappresentare l'insieme di tutti i possibili \emph{token}.
\begin{example} Si assumano i token come caratteri dell'alfabeto. Sia $k=2$
    
    Per i documenti $D_1 = abcab, D_2 = ccadf$, gli insiemi di \emph{2-shingle} corrispondenti sono 
    \[
        S(D_1) = \{ab,bc,ca\} \quad S(D_1) = \{cc,ca,ad,df\}
    \]
    
\end{example}

Ogni documento $D \in U$ può quindi essere rappresentato come l' insieme dei suoi \emph{k-shingle} $C = S(D)$. In modo più pratico ogni documento è rappresentato come $C=S(D)$ un vettore binario avente ogni possibile elemento dell'insieme $U^k$. Codificando gli insiemi come vettori binari è possibile definire l'intersezione come \emph{bitwise and} e l'unione come \emph{bitwise or}.È facile notare che questi vettori sono molto grandi, ma conservano i dati in modo sparso.

\vspace{1em}
È inoltre facile rappresentare la $\jsim$ con un'interpretazione probabilistica
\[
    \jsim(C_1, C_2) = \frac{C_1 \cap C_2}{C_1 \cup C_2} = \Pr[C_1 \cap C_2 | C_1 \cup C_2].
\]

Concludendo i \emph{k-shingle} permettono di rappresentare il contenuto dei documenti attraverso degli elementi di un insieme. Questa tecnica permette inoltre di trovare documenti simili anche per testi che appaiono in ordine diverso.

L'insieme di tutti i dati gestiti è visibile come una matrice $M \in {0,1}^{N\times m}$ con $N$ numero di documenti e $m = |U|^k$ \emph{shingles} possibili. Si presenta un esempio di seguito
\input{figure/matrice_doc.tex}

\vspace{1em}\noindent
Si passa ora alla tecnica del Min-hashing che permette di rendere i grandi insiemi di \emph{k-shingles} in piccole firme mantenendo la misura di similarità
\subsubsection{Min-hashing}
Si presenta un esempio per mostrare perché il min-hashing è necessario
\begin{example}
    Sia $N = 10^6$ il numero di documenti, per cui si hanno $10^6$ \emph{k-shingles}, e si ottengono $\frac{(10^6)^2}{2} = 5\cdot10^{11}$ confronti.
\end{example}
La tecnica precedente permette solo una rappresentazione robusta dei documenti, con il min-hashing si cerca un modo per trovare documenti simili tramtite delle piccole firme. 

La difficoltà consiste nel definire una buona firma che conservi la similarità tra due insiemi.
Per cui si cerca una funzione hash, da applicare agli insiemi \emph{shingle} (colonne della matrice $M$ vista in precedenza). La funzione hash cercata deve avere le seguenti proprietà
\begin{enumerate}
    \item $h(C)$ deve essere abbastanza piccolo da poter essere mantenuto in memoria
    \item $\jsim(C_1,C_2)$ deve essere ``vicino'' all'equivalenza delle hash.
    (se $\jsim(C_1,C_2)$ è alto, con alta probabilità $h(C_1) = h(C_2)$, viceversa se il valore di $\jsim(C_1,C_2)$ è basso, con alta probabilità $h(C_1) \neq h(C_2)$).
\end{enumerate}

A livello pratico si ``mandano'' i documenti in \emph{bucket}, due documenti simili devono finire nello stesso \emph{bucket}.

Ovviamente la funzione hash cercata deve dipendere in qualche modo dalla misura di distanza, non tutte li funzioni hash funzionano su tutte le misure di distanza.

La funzione hash della Jaccard Similarity è chiamata \emph{Min-Hashing}. Per ogni documento $D$ (colonna $C$ della matrice caratteristica $M$), per una funzione di permutazione $\pi$ si definisce la funzione $\text{Min\_Hashing}(C)_\pi = $ indice della prima riga nella quale $C$ ha valore 1, chiamato $\min(\pi(C))$. Per $\pi$ permutazione qualsiasi.

\vspace{1em}\noindent
Un problema da presentare è quello della generazione di una permutazione $\pi$ qualsiasi, non è un compito computazionalmente facile, nella pratica si fa uso di funzioni hash casuali, che permettono di approssimare il comportamento di una permutazione casuale.

Si definisce nel dettaglio un esempio. Sia $M$ la matrice caratteristica del \emph{dataset}, $\Pi$ una permutazione scelta casualmente. Sia $m$ il numero di righe (\emph{k-shingles}) di $M$. Per una colonna $C$ di $M$ si definisce la funzione \emph{MinHash} $h_\pi(C)$ nel seguente modo
\[
    h_\pi(C) = \min \{ i \in [m]: C_\pi[i] = 1\} = \min(\pi(C)).
\]

\input{figure/min_hash.tex}
Nella in figura \ref{fig:3} il documento $D_1$ ha i valori `1' per gli \emph{shingle} $\{1,2,3,4\}$, applicando le diverse permutazioni si ottiene
\begin{align*}    
    \pi_1(D_1) = \{2,3,5,4\} \quad \min = 2\\
    \pi_2(D_1) = \{4,2,5,4\} \quad \min = 2\\
    \pi_3(D_1) = \{3,4,1,5\} \quad \min = 1\\
\end{align*}


Il MinHashing ha una complessità spaziale $h_\pi(C) = \Theta(\log m)$ con $|C| = |U|^k = m$
\newpage
Si presenta una dimostrazione formale della proprietà di preservazione della similarità del MinHash
\begin{theorem}
    Siano $C_1,C_2$ fissati sia scelta una permutazione $\Pi$ in modo uniforme allora
    \[
        \Pr[h_\pi(C_1) = h_\pi(C_2)] = \jsim(C_1,C_2)
    \]
\end{theorem}
\begin{proof}
    Sia $X$ un documento $y \in X$ è uno dei suoi shingle allora $\Pr[\pi(y) \min \pi(X)] = \frac{1}{|X|}$ poiché è equiprobabile per qualsiasi riga di essere l'indice del minimo
    \vspace{1em}
    Sia $y$ tale che $\pi(y) = \min(\pi(C_1 \cap C_2))$ $\pi$ è uniforme casuale allora
    \begin{align*}
        \pi(y) = \min(\pi(C_1)) \text{ se $y \in C_1$} \\
        \pi(y) = \min(\pi(C_2)) \text{ se $y \in C_2$}
    \end{align*}
    La probabilità che entrambi siano veri è 
    \[
        \Pr[\underbrace{\min(\pi(C_1)}_{h(C_1)} = \underbrace{\min(\pi(C_2))}_{h(C_2)})] = \frac{|C_1 \cap C_2|}{|C_1 \cup C_2|}
    \]
    Ovvero proprio la $\jsim(C_1,C_2)$
\end{proof}
Da questo teorema si può sfruttare la concentrazione del valore atteso per aumentare la confidenza. Usando MinHash mutualmente indipendenti $SIG(C) = \langle h_{\pi_1}(C),h_{\pi_2}(C),\dots, h_{\pi_t}(C) \rangle$

\vspace{1em}\noindent
Date queste premesse, si definisce la misura di similarità sulle firme descritte
\begin{definition}[Similarità tra firme]
    $\text{Sign-Sim}(C_1,C_2)$ è la funzione di similarità tra due vettori $\text{SIG}(C_1)$ e $\text{SIG}(C_2)$, dei due vettori firma \text{Sign}$(C_1)$ e \text{Sign}$(C_2)$ dove
    \[
        \text{Sign-Sim}(C_1,C_2) = \text{frazione delle firme MinHash in cui coincidono}.
    \]
    In modo pù formale è possibile definire una variabile aleatoria $Z$ che assume i segeunti valori
    \[
        Z = \begin{cases*}
            1 \text{ se} \min(\pi(C_1)) = \min(\pi(C_2))\\
            0 \text{ altrimenti}
        \end{cases*}
    \]
    la Sign-Sim$(C_1,C_2)$ è equivalente a $\Pr[Z=1]$ dal teorema precedente.
\end{definition}

\vspace{1em}\noindent
Qui di seguito si riporta un esempio di come il MinHashing conservi la similarità
\begin{example}
    Siano $\pi_1,\pi_2,\pi_3$ tre permutazioni casuali, e $M$ la matrice in input come nella figura \ref{fig:3}
    \input{figure/min_hash_similarity.tex}
\end{example}

\vspace{1em}\noindent
Di seguito è riportato il codice che calcola la similarità tra le firme di due documenti

\input{algorithms/doc-pair.tex}

\vspace{1em}\noindent
Si nota che $\jsim(C_1,C_2) \neq \text{Sign-Sim}(C_1,C_2)$, ma per il teorema principale del MinHashing $\jsim(C_1,C_2) = \mathbb{E}_{\pi}\left[{\text{Sign-Sim}(C_1,C_2)}\right]$ si ottiene il corollario:

\begin{corollary}
    \text{Sign-Sim}$(C_1,C_2) \longrightarrow \jsim(C_1,C_2)$ per $t \to \infty$.
\end{corollary}
\vspace{1em}\noindent
Inoltre le $t$ funzioni hash sono mutualmente indipendenti, per cui si ha la concentrazione uguale a $|\text{Sign-Sim}(C_1,C_2) - \jsim(C_1,C_2)|$.

\subsubsection*{Analisi complessità spaziale del MinHashing}
Si scelgano $t = 100$ funzioni permutazioni casuali delle $m$ righe, $Sig(i,C)$ è l'indice della prima riga contenente `1' nella colonna $C$, permutata con l'\emph{i-esima} permutazione. \textbf{Sig$(i,C)$} è un indice, per cui occupa $\Theta(\log(|C|)) = \Theta(\log(m))$ spazio.

\subsubsection*{Generazione di permutazioni casuali}
È ora importante notare che le funzioni di MinHash $h_{\pi}(C)$ fanno uso di tante permutazioni casuali sull'insieme $U^k (m = |U|^k)$, generare permutazioni è un problema computazionalmente impegnativo, per cui le permutazioni vengono sostituite da $t$ funzioni hash casuali $h:[m]\to[m]$, la probabilità di 
collisione è bassa, le collisioni vengono ignorate.

\vspace{1em}
Di seguito l'algoritmo efficiente per il calcolo della matrice delle firme come in figura \ref*{fig:3}, l'algoritmo fa uso di funzioni hash casuali per risolvere il problema delal generazione di permutazioni

\input{algorithms/sig-matrix.tex}
\subsubsection*{Analisi della complessità}
L'algoritmo esegue $\Theta(mN)$ iterazioni, ogni iterazione richiede $t$ calcoli hash $f_i(i)$ se la condizione $M(j,C)$ è verificata.

\vspace{1em}\noindent
Tutte le tecniche impiegate fin'ora non hanno permesso di superare la barriera nel numero di confronti in $O(n^2)$ per determinare le coppie di documenti simili, si presenta quindi la seguente tecnica
\subsubsection{Local Sensitive Hashing}
