In questa sezione si parla di problemi per cui la dimensione dei dati è eccessiva, ad esempio si potrebbe essere interessati a valutare uno dei seguenti problemi
\begin{enumerate}
    \item \textit{Pagine con una grande porzione di parole simili}
    \item \textit{Clienti con acquisti simili}
    \item \textit{Immagini con elementi simili}
\end{enumerate}
\begin{example}
    Input: punti ad alte dimensioni $x_1,x_2,\dots,x_N$
    \[
        \begin{bmatrix}
            1 & 4 & 1\\
            0 & 2 & 1\\
            0 & 1 & 0\\
        \end{bmatrix}
        \implies
        \begin{bmatrix}
            1 & 4 & 1 & 0 & 2 & 1 & 0 & 1 & 0
        \end{bmatrix} \in \{ 0,1,\dots,c\}^h.
    \] per $h$ molto grande
    Si introduce una funzione di distanza $d(x_1,x_2)$ che quantifichi quanto i due dati sono \emph{vicini}.
    Un'applicazione comune consiste nel trovare tutte le coppie di dati in un insieme di dati che sono ``abbastanza'' vicini (rispetto a una soglia $s$) per cui $d(x_1,x_2) \leq s$.

    Una soluzione deterministica banale richiede tempo $O(N^2\cdot h)$ con $N$ numero di punti e $h$ dimensione dei dati. Banalmente si calcola la distanza per ogni coppia possibile $\binom{n}{2} = O (n^2)$.
\end{example}

È possibile trovare coppie di dati simili in tempo $O(N \cdot h')$ con $h' << h$, come spiegato di seguito.
\subsection{Document Similarity}
Dato un grande insieme di documenti $U^*$ grande $N \approx 10^9$, si vogliono trovare i documenti simili. Le applicazioni possibili riguardano la ricerca di siti web duplicati e la ricerca di notizie simili.

Il problema descritto presenta diverse sfide 
\begin{itemize}
    \item molti pezzi piccoli di un documento possono essere in ordine diverso
    \item il numero di documenti ($N$) è troppo elevato per confrontare tutte le coppie
    \item i documenti possono essere talmente grandi ($h$) da non poter essere mantenuti in memoria 
\end{itemize}

È inoltre necessario definire una funzione di distanza adeguata, a tale scopo si fa uso della \emph{Jaccard Similarity} definita su due insiemi $C_1,C_2$ come segue
\[
    \text{Jaccard similarity}(C_1,C_2) = \frac{|C_1 \cap C_2|}{|C_1 \cup C_2|}
\]
Si definisce anche un'altra misura \emph{Jaccard Distance} $d(C_1,C_2) = 1- \jsim(C_1,C_2)$. Resta ora da definire come applicare la Jaccard Similarity a due documenti (stringhe di lunghezza finita). 

Si definiscono i punti chiave della risoluzione del problema
\begin{enumerate}
    \item Input: Un universo $U$ enorme di documenti
    \item Shingling: conversione dei documenti in \emph{grandi insiemi}
    \item Min-Hashing: conversione di grandi insiemi in \emph{piccole firme} mantenendo la $\jsim$
    \item Local-Sensitive-Hashing: Individuare le firme che potrebbero essere simili 
    \item Output: coppie di documenti candidati 
\end{enumerate}

Restano ora da definire nel dettaglio le tecniche descritte
\subsubsection{Shingling}
Facciamo uso della tecnica di \emph{Shingling} per convertire i documenti in insiemi. Un \emph{k-shingle} per un documento è una sequenza di $k$ token che appaiono in un documento. I \emph{token} possono generalmente essere definiti come: caratteri o parole, si fa uso di $U_t$ per rappresentare l'insieme di tutti i possibili \emph{token}.
\begin{example} Si assumano i token come caratteri dell'alfabeto. Sia $k=2$
    
    Per i documenti $D_1 = abcab, D_2 = ccadf$, gli insiemi di \emph{2-shingle} corrispondenti sono 
    \[
        S(D_1) = \{ab,bc,ca\} \quad S(D_1) = \{cc,ca,ad,df\}
    \]
    
\end{example}

Ogni documento $D \in U$ può quindi essere rappresentato come l' insieme dei suoi \emph{k-shingle} $C = S(D)$. In modo più pratico ogni documento è rappresentato come $C=S(D)$ un vettore binario avente ogni possibile elemento dell'insieme $U^k$. Codificando gli insiemi come vettori binari è possibile definire l'intersezione come \emph{bitwise and} e l'unione come \emph{bitwise or}.È facile notare che questi vettori sono molto grandi, ma conservano i dati in modo sparso.

\vspace{1em}
È inoltre facile rappresentare la $\jsim$ con un'interpretazione probabilistica
\[
    \jsim(C_1, C_2) = \frac{C_1 \cap C_2}{C_1 \cup C_2} = \Pr[C_1 \cap C_2 | C_1 \cup C_2].
\]

Concludendo i \emph{k-shingle} permettono di rappresentare il contenuto dei documenti attraverso degli elementi di un insieme. Questa tecnica permette inoltre di trovare documenti simili anche per testi che appaiono in ordine diverso.

L'insieme di tutti i dati gestiti è visibile come una matrice $M \in {0,1}^{N\times m}$ con $N$ numero di documenti e $m = |U|^k$ \emph{shingles} possibili. Si presenta un esempio di seguito
\input{figure/matrice_doc.tex}

\vspace{1em}\noindent
Si passa ora alla tecnica del Min-hashing che permette di rendere i grandi insiemi di \emph{k-shingles} in piccole firme mantenendo la misura di similarità
\subsubsection{Min-hashing}
Si presenta un esempio per mostrare perché il min-hashing è necessario
\begin{example}
    Sia $N = 10^6$ il numero di documenti, per cui si hanno $10^6$ \emph{k-shingles}, e si ottengono $\frac{(10^6)^2}{2} = 5\cdot10^{11}$ confronti.
\end{example}
La tecnica precedente permette solo una rappresentazione robusta dei documenti, con il min-hashing si cerca un modo per trovare documenti simili tramtite delle piccole firme. 

La difficoltà consiste nel definire una buona firma che conservi la similarità tra due insiemi.
Per cui si cerca una funzione hash, da applicare agli insiemi \emph{shingle} (colonne della matrice $M$ vista in precedenza). La funzione hash cercata deve avere le seguenti proprietà
\begin{enumerate}
    \item $h(C)$ deve essere abbastanza piccolo da poter essere mantenuto in memoria
    \item $\jsim(C_1,C_2)$ deve essere ``vicino'' all'equivalenza delle hash.
    (se $\jsim(C_1,C_2)$ è alto, con alta probabilità $h(C_1) = h(C_2)$, viceversa se il valore di $\jsim(C_1,C_2)$ è basso, con alta probabilità $h(C_1) \neq h(C_2)$).
\end{enumerate}

A livello pratico si ``mandano'' i documenti in \emph{bucket}, due documenti simili devono finire nello stesso \emph{bucket}.

Ovviamente la funzione hash cercata deve dipendere in qualche modo dalla misura di distanza, non tutte li funzioni hash funzionano su tutte le misure di distanza.

La funzione hash della Jaccard Similarity è chiamata \emph{Min-Hashing}. Per ogni documento $D$ (colonna $C$ della matrice caratteristica $M$), per una funzione di permutazione $\pi$ si definisce la funzione $\text{Min\_Hashing}(C)_\pi = $ indice della prima riga nella quale $C$ ha valore 1, chiamato $\min(\pi(C))$. Per $\pi$ permutazione qualsiasi.

\vspace{1em}\noindent
Un problema da presentare è quello della generazione di una permutazione $\pi$ qualsiasi, non è un compito computazionalmente facile, nella pratica si fa uso di funzioni hash casuali, che permettono di approssimare il comportamento di una permutazione casuale.

Si definisce nel dettaglio un esempio. Sia $M$ la matrice caratteristica del \emph{dataset}, $\Pi$ una permutazione scelta casualmente. Sia $m$ il numero di righe (\emph{k-shingles}) di $M$. Per una colonna $C$ di $M$ si definisce la funzione \emph{MinHash} $h_\pi(C)$ nel seguente modo
\[
    h_\pi(C) = \min \{ i \in [m]: C_\pi[i] = 1\} = \min(\pi(C)).
\]

\input{figure/min_hash.tex}
Nella in figura \ref{fig:3} il documento $D_1$ ha i valori `1' per gli \emph{shingle} $\{1,2,3,4\}$, applicando le diverse permutazioni si ottiene
\begin{align*}    
    \pi_1(D_1) = \{2,3,5,4\} \quad \min = 2\\
    \pi_2(D_1) = \{4,2,5,4\} \quad \min = 2\\
    \pi_3(D_1) = \{3,4,1,5\} \quad \min = 1\\
\end{align*}


Il MinHashing ha una complessità spaziale $h_\pi(C) = \Theta(\log m)$ con $|C| = |U|^k = m$
\newpage
Si presenta una dimostrazione formale della proprietà di preservazione della similarità del MinHash
\begin{theorem}
    Siano $C_1,C_2$ fissati sia scelta una permutazione $\Pi$ in modo uniforme allora
    \[
        \Pr[h_\pi(C_1) = h_\pi(C_2)] = \jsim(C_1,C_2)
    \]
\end{theorem}
\begin{proof}
    Sia $X$ un documento $y \in X$ è uno dei suoi shingle allora $\Pr[\pi(y) \min \pi(X)] = \frac{1}{|X|}$ poiché è equiprobabile per qualsiasi riga di essere l'indice del minimo
    \vspace{1em}
    Sia $y$ tale che $\pi(y) = \min(\pi(C_1 \cap C_2))$ $\pi$ è uniforme casuale allora
    \begin{align*}
        \pi(y) = \min(\pi(C_1)) \text{ se $y \in C_1$} \\
        \pi(y) = \min(\pi(C_2)) \text{ se $y \in C_2$}
    \end{align*}
    La probabilità che entrambi siano veri è 
    \[
        \Pr[\underbrace{\min(\pi(C_1)}_{h(C_1)} = \underbrace{\min(\pi(C_2))}_{h(C_2)})] = \frac{|C_1 \cap C_2|}{|C_1 \cup C_2|}
    \]
    Ovvero proprio la $\jsim(C_1,C_2)$
\end{proof}
Da questo teorema si può sfruttare la concentrazione del valore atteso per aumentare la confidenza. Usando MinHash mutualmente indipendenti $SIG(C) = \langle h_{\pi_1}(C),h_{\pi_2}(C),\dots, h_{\pi_t}(C) \rangle$

\vspace{1em}\noindent
Date queste premesse, si definisce la misura di similarità sulle firme descritte
\begin{definition}[Similarità tra firme]
    $\text{Sign-Sim}(C_1,C_2)$ è la funzione di similarità tra due vettori $\text{SIG}(C_1)$ e $\text{SIG}(C_2)$, dei due vettori firma \text{Sign}$(C_1)$ e \text{Sign}$(C_2)$ dove
    \[
        \text{Sign-Sim}(C_1,C_2) = \text{frazione delle firme MinHash in cui coincidono}.
    \]
    In modo pù formale è possibile definire una variabile aleatoria $Z$ che assume i segeunti valori
    \[
        Z = \begin{cases*}
            1 \text{ se} \min(\pi(C_1)) = \min(\pi(C_2))\\
            0 \text{ altrimenti}
        \end{cases*}
    \]
    la Sign-Sim$(C_1,C_2)$ è equivalente a $\Pr[Z=1]$ dal teorema precedente.
\end{definition}

\vspace{1em}\noindent
Qui di seguito si riporta un esempio di come il MinHashing conservi la similarità
\begin{example}
    Siano $\pi_1,\pi_2,\pi_3$ tre permutazioni casuali, e $M$ la matrice in input come nella figura \ref{fig:3}
    \input{figure/min_hash_similarity.tex}
\end{example}

\vspace{1em}\noindent
Di seguito è riportato il codice che calcola la similarità tra le firme di due documenti

\input{algorithms/doc-pair.tex}

\vspace{1em}\noindent
Si nota che $\jsim(C_1,C_2) \neq \text{Sign-Sim}(C_1,C_2)$, ma per il teorema principale del MinHashing $\jsim(C_1,C_2) = \mathbb{E}_{\pi}\left[{\text{Sign-Sim}(C_1,C_2)}\right]$ si ottiene il corollario:

\begin{corollary}
    \text{Sign-Sim}$(C_1,C_2) \longrightarrow \jsim(C_1,C_2)$ per $t \to \infty$.
\end{corollary}
\vspace{1em}\noindent
Inoltre le $t$ funzioni hash sono mutualmente indipendenti, per cui si ha la concentrazione uguale a $|\text{Sign-Sim}(C_1,C_2) - \jsim(C_1,C_2)|$.

\subsubsection*{Analisi complessità spaziale del MinHashing}
Si scelgano $t = 100$ funzioni permutazioni casuali delle $m$ righe, $Sig(i,C)$ è l'indice della prima riga contenente `1' nella colonna $C$, permutata con l'\emph{i-esima} permutazione. \textbf{Sig$(i,C)$} è un indice, per cui occupa $\Theta(\log(|C|)) = \Theta(\log(m))$ spazio.

\subsubsection*{Generazione di permutazioni casuali}
È ora importante notare che le funzioni di MinHash $h_{\pi}(C)$ fanno uso di tante permutazioni casuali sull'insieme $U^k (m = |U|^k)$, generare permutazioni è un problema computazionalmente impegnativo, per cui le permutazioni vengono sostituite da $t$ funzioni hash casuali $h:[m]\to[m]$, la probabilità di 
collisione è bassa, le collisioni vengono ignorate.

\vspace{1em}
Di seguito l'algoritmo efficiente per il calcolo della matrice delle firme come in figura \ref*{fig:3}, l'algoritmo fa uso di funzioni hash casuali per risolvere il problema delal generazione di permutazioni

\input{algorithms/sig-matrix.tex}
\subsubsection*{Analisi della complessità}
L'algoritmo esegue $\Theta(mN)$ iterazioni, ogni iterazione richiede $t$ calcoli hash $f_i(i)$ se la condizione $M(j,C)$ è verificata.

\vspace{1em}\noindent
Tutte le tecniche impiegate fin'ora non hanno permesso di superare la barriera nel numero di confronti in $\\Theta(n^2)$ per determinare le coppie di documenti simili, si presenta quindi la seguente tecnica
\subsubsection{Local Sensitive Hashing}
Con la tecnica del Local Sensitive Hashing si fa uso di una funzione $f(x,y)$, che permette di determinare se $(x,y)$ è una possibile coppia di cui la similarità va successivamente verificata.

\vspace{1em}\noindent
Si definisce $f(x,y)$, si consideri la matrice del MinHashing $SIG(i,C)\ i=1,\dots,t \; C=1,\dots,N$. Viene calcolato il valore hash delle colonne di $SIG(i,C)$, ogni coppia il cui valore hash è lo stesso è una \emph{coppia candidata}.

\vspace{1em}\noindent
Si presenta il procedimento in modo più formale,
\begin{definition}
    Sia definito $s$ un valore di similarità tale che $0<s<1$. Le colonne $X,Y$ di $SIG(*,*)$ sono dette \emph{candidate} se le firme MinHash ($SIG(*,X)$ e $SIG(*,Y)$) sono identiche per almeno una frazione $s$ delle loro righe:
    \[
        \frac{|\{ i \in [t] \mid SIG(i,X) = SIG(i,Y) \}|}{t} \geq s
    \]
\end{definition}
Grazie al corollario precedente la similarità dei documenti è ``conservata'' dalle firme. \textit{Nota:} la similarità tra le firme è data da svariati valori identici tra loro.

L'idea generale consiste nell'eseguire, tante volte, le hash delle colonne in $SIG(*,*)$, così facendo si ottengono dei \emph{bucket} in cui finiscono le firme dei documenti simili. Solo le coppie candidate hanno buona probabilità di collisione hash, quindi di finire nello stesso \emph{bucket}.

\subsubsection*{Problemi tecnici}
Calcolare l'hash completa di un documenti in un \emph{bucket} può portare a falsi positivi e falsi negativi per le coppie candidate.

Per risolvere il problema, ogni firma (colonna di $SIG(*,*)$) viene partizionata in un insieme di \emph{bande}. La matrice $SIG(*,*)$ viene divisa in $b$ bande ogniuna composta da $r$ righe. Per ogni banda, si esegue l'hash \emph{LSH} in $c$ diversi \emph{bucket} ($c$ deve essere abbastanza grande rispetto a $r$, per evitare collisioni di firme diverse). 

\vspace{1em}\noindent
Il nuovo criterio per determinare le coppie di documenti simili consiste nel selezionare le firme che finiscono nello stesso \emph{bucket} per \emph{almeno} una banda.

\vspace{1em}\noindent
Dato questo nuovo criterio più preciso rimane da determinare i valori della variabili $b,r$ (e $t = br$) per ottenere una buona candidatura (tante coppie simili e poche collisioni di firme non simili).

Per valutare la correttezza dell'algoritmo si assuma di fare uso di \emph{perfect hashing} in cui le collisioni derivano dalla \emph{bande} identiche.

\begin{example}
    Si suppongano $10^5$ colonne ($N = 10^5$) in $SIG(*,*)$, le firme di 100 righe ($t=100$), date queste premesse, la tabella delle firme occupa \textit{40Mb}.

    Si scelgano $b=20$ bande ogniuna di $r=5$ interi.
    L'obiettivo consiste nel trovare le coppie di documenti la cui similarità $\geq s$ per $s=0.8$ dato in input. 

    \begin{enumerate}
        \item[Caso 1.] Siano $C_1,C_2$ due documenti $80\%$ simili, si ha $\jsim(C_1,C_2) = 0.8$. Poiché $\jsim(C_1,C_2) \geq s$ allora $(C_1,C_2)$ deve essere una coppia candidata, per cui le hash devono avere almeno una \emph{banda} in comune.
        
        La probabilità che $C_1,C_2$ abbiano una \emph{banda} identica è $(0.8)^{5} = 0.328$ (tutti i 5 interi combaciano), mentre la probabilità che $C_1,C_2$ non abbiano \emph{nessuna} banda in comune è $(1-0.328)^{20} = 0.00035$ (nessuna delle 20 bande soddisfa la probabilità precedente).
        
        \noindent
        Per cui il $99.965\%$ delle coppie $80\%$ simili vengno correttamente identificate.

        \item[Caso 2.] Siano $C_1,C_2$ due documenti $30\%$ simili, si ha $\jsim(C_1,C_2) = 0.3$. Poiché $\jsim(C_1,C_2) < s$ allora $(C_1,C_2)$ non deve essere una coppia candidata. 
        
        Ovvero tutte le bande devono essere differenti
        La probabilità che $C_1,C_2$ abbiano una \emph{banda} identica è $(0.3)^{5} = 0.00243$ (tutti i 5 interi combaciano), mentre la probabilità che $C_1,C_2$ non siano identici in \emph{nessuna} banda per tutte le 20 è $1 - (1-0.00243)^{20} = 0.0474$.

        Per cui la probabilità di falsi positivi è $4.74\%$ per documenti con similarità $0.3$.
    \end{enumerate}

\end{example}
Ne segue un'analisi dell'efficacia di \emph{LSH}. 
\newpage
Nella figura \ref{fig:4} viene rappresentata una funzione costante per il \emph{threshold} $s$, in cui tutte le coppie di documenti $C_1,C_2$ che hanno una similarità $< s$ vengono scartate, e le coppie di cui la similarità $\geq s$ vengono sempre individuate. Non ci sono falsi positivi ne falsi negativi.
\input{figure/grafico1.tex}

\noindent
Impostando la variabile $r=1$, ovvero tutte bande di un solo elemento, la funzione è lineare come rappresentato nella figura \ref{fig:5}. Figura in cui non è presente un comportamento che separa chiaramente le coppie sopra il \emph{threshold}. La probabilità di selezionare una coppia $(C_1,C_2)$ è proporzionale alla $\jsim(C_1,C_2)$ (Per il teorema del MinHashing).
\input{figure/grafico2.tex}

Facendo uso della tecnica delle \emph{bande} permette di ottenere un comportamento che approssima l'ottimo rappresentato in \ref{fig:4}. Suppondendo una coppia di documenti $C_1,C_2$, per cui $\jsim(C_1,C_2) = x$ facendo uso della tecnica delle \emph{bande} si ottiene la probabilità che tutti gli elementi nella banda sono identici $= x^r$, e la probabilità che esista un elemento nella banda diverso $= 1 - x^r$.
Per cui si ha 
\begin{align*}
    &\Pr[\text{Non esistenza di una band identica}] = (1-x^r)^b\\
    &\Pr[\text{Esistenza di una band identica}] = 1 - (1-x^r)^b
\end{align*}
si è interessati all'ultima probabilità presentata, che ha la proprietà di aumentare con $b$ e diminuire con $r$. 

\newpage \noindent
Aumentare troppo una delle due variabili può portare a troppi falsi positivi o falsi negativi. È importante trovare un punto ottimale.

\vspace{1em}\noindent
Per un parametro $s$ fissato si deve segliere $t$ ovvero il numero di MinHash (numero di righe di $SIG(*,*)$), il numero di \emph{bande} $b$ e di conseguenza il numero $r$ di elementi per banda.

\input{figure/grafico3.tex}
Impostando i parametri $b$ e $r$, si ha un punto di flesso a $k = \left(\frac{1}{b}\right)^\frac{1}{r}$. Un idea efficace è quella di impostare $b,r$ tali che $b\cdot r \leq t$ e $s \approx \left(\frac{1}{b}\right)^\frac{1}{r}$. Come è possibile vedere in figura \ref{fig:6} cambiare i parametri $r$ e $b$ permette di approssimare molto bene un algoritmo ideale.

\input{figure/grafico4.tex}
L'area verde sottostante la curva rappresenta il numero di falsi positivi, mentre l'area blu rappresenta il numero di falsi negativi.

%TODO: Capire se argomentare oltre
\newpage
Per introdurre i prossimi problemi è necessario introdurre il concetto di Data Stream
\input{data_stream.tex}

\subsection{Pattern Matching}
Si presenta il problema del pattern matching. Sia $\Sigma$ un alfabeto, data una stringa $y = y_{1}y_{2}\dots y_{n} \in \Sigma^{n}$, e una Stream $S \in \Sigma^m$ con $n<m$, determinare il numero di volte che $y$ appare in $S$, considerando $m$ e possibilmente anche $n$ molto grandi.

\vspace{1em}\noindent
Per risolvere il problema del Pattern Matching è necessario uno \emph{sketch} per l'identità tra stringhe, tra cui la \emph{funzione hash di Rabin}
\begin{definition}[Funzione hash di Rabin]
    Sia $q$ un numero primo tale che $q > |\Sigma|$ e $z \in_u [0,q)$ Sia $x[1,n] \in \Sigma^{n}$ una stringa di lunghezza $n$. La funzione hash di Rabin è definita
    \[
        K_{qz}(x) = \left( \sum_{i=1}^{n}x[i]\cdot z^{n-i}\right) \mod q
    \]
    Ovvero $K_{qz}$ è un polinomio in $\mathbb{F}_q$ valutato in $z$ con coefficienti i caratteri di $x$.
\end{definition}

Si definisce ora lo \emph{sketch} di Rabin
\begin{definition}[Sketch di Rabin]
    Si definisce il Rabin \emph{sketch} la coppia 
    \[
        f(x) = (K_{qz}(x), z^{|x|} \mod q)
    \]
    $f(x)$ fa uso di $O(\log q)$ bit, facendo uso di $q = n^{O(1)}$ $O(\log n)$ bit di spazio.
\end{definition}

\begin{lemma}[Concatenazione di caratteri]
    $K_{qz}(x)$ è facile da aggiornare. Si supponga di avere $K_{qz}(x)$ e un carattere $c$, è facile calcolare $K_{qz}(x\cdot c)$: 
    \[
        K_{qz}(x\cdot c) = (K_{qz}(x) + z\cdot c) \mod q
    \]
    La lunghezza di $x\cdot c$ è $|x|+1$ e $z^{|x|+1} \mod q = (z^{|x|} \mod q)\cdot z \mod q$. 
\end{lemma}
\vspace{2em}
\begin{lemma}[Concatenazione di stringhe]
    Si supponga di avere $K_{qz}(x)$ e una stringa $y$, è facile calcolare $K_{qz}(x\cdot y)$: 
    \[
        K_{qz}(x\cdot y) = (K_{qz}(x) \cdot z^{|y|} +  K_{qz}(y)) \mod q
    \]
    La lunghezza di $x\cdot y$ è $|x|+|y|$ e $z^{|x|+|y|} \mod q = (z^{|x|} \mod q)\cdot (z^{|y|} \mod q)$. 
\end{lemma}
\vspace{2em}
Inoltre un'altra proprietà dello \emph{sketch}: se $x \neq y$ allora $K_{qz}(x) \neq K_{qz}(y)$ con alta probabilità.

\begin{lemma}
    Siano $x,y$ due stringhe tali che $x \neq y$ con $\max(|x|,|y|) = n$ allora
    \[
        \Pr[K_{qz}(x) = K_{qz}(y)] \leq \frac{n}{q}
    \]
\end{lemma}
\begin{proof}
    \[
        \Pr[K_{qz}(x) = K_{qz}(y)] = \Pr[ \underbrace{K_{qz}(x) - K_{qz}(y)} \equiv_q 0]
    \]
    $K_{qz}(x) - K_{qz}(y)$ è a sua volta un polinomio in $\mathbb{F}_q$, sia $x-y$ la stringa tale che $(x-y)[i]$ = $x[i]-y[i] \mod q$ (si aggiungano dei \emph{leading zeros} alla stringa più corta).
    
    \vspace{1em}
    È facile notare che $K_{qz}(x) - K_{qz}(y) \mod q = K_{qz}(x-y)$ per $x \neq y$, $K_{qz}(x-y)$ è un polinomio di grado $\leq n$ su $\mathbb{F}_q$, per cui le radici sono al massimo $n$, gli elementi di $\mathbb{F}_q$ sono $q$, quindi si ottiene la probabilità di prendere una radice è $\leq \frac{n}{q}$.
\end{proof}
Nel seguente codice si mostra la costruzione del poliomio per una data stringa $x=x_1,\dots,x_n$
\input{algorithms/karps-rabin.tex}

\vspace{1em}\noindent
L'algoritmo con input $(q,z,x)$ richiede spazio $O(\log q)$, mentre il procedimento per la costruzione del polinomio di $x$ e $y$ concatenate richiede spazio $O(\log q + \log(|y|) \log q)$.

\begin{corollary}[probabilità di errore]
    Sia $q$ un primo, tale che $n^{c+1} \leq q \leq 2n^{c+1}$ per qualche $c>0$ allora per ogni $x\neq y$,
    \[
        Pr{K_{qz}(x) = K_{qz}(y) \mod q} \leq \frac{1}{n^c}
    \]
\end{corollary}
\noindent
La dimensione dello \emph{sketch} è $O(\log n)$ contro $O(n)$ iniziale.

\vspace{2em}\noindent
Ritornando al problema del pattern matching, si ricorda il problema: data una stringa $y$ contare quante volte appare nel Data Stream con alfabeto $\Sigma = [s]$, la Stream è una stringa $x<x_1,x_2,\dots,x_m> \in \Sigma^m$ e $y = <y_1,y_2,\dots,x_m> \in \Sigma^n$.

\vspace{1em}
Si suppone di aver letto i caratteri dalla Stream $<x_1,x_2,\dots,x_i>$ con $i\geq n$, quindi sono stati già calcoli gli \emph{sketch} $K_{qz}(<x_{n-i+1},\dots,x_i>)$ e $K_{qz}(y)$, quando un nuovo elemento della Stream viene ricevuto $x_{i+1}$ si calcola il nuovo \emph{sketch}
\[
    K_{qz}(<x_{i-n+2},\dots, x_{i+2}>)= ( K_{zq}(<x_{i-n+1},\dots, x_{i}>) - x_{i-n+1}\cdot z^{n-1}) z + x_{i+1}) \mod q
\]
in modo più pratico, viene tolto l'elemento meno recente $x_{i-n+1}$ e viene inserito il nuovo elemento $x_{i+1}$.

Si procede ripetendo con $i$ aumentato di 1.

\subsubsection*{Analisi delle prestazioni}
\begin{lemma}[Complessità spaziale] Per una Stream lunga $m$, un pattern lungo $n$ lo spazio occupato dall'algoritmo è $\Theta(n)$, è facile da verificare vedendo che è necessario mantenere gli ultimi $n$ elementi della Stream per toglierli al momento giusto.
\end{lemma}

\begin{lemma}[Complessità temporale] Il numero di operazioni per elemento è $O(1)$, aggiornare lo \emph{sketch} richiede un numero costante di operazioni, è sufficiente precalcolare $z^{n-1} \mod q$.
\end{lemma}

Un migliore algoritmo è quello di \emph{Porat-Porat} che occupa $O(\log n)$ spazio.

\subsection{Proprietà algoritmi su Stream}
Dal momento che non è possibile memorizzare l'intera Stream è necessario mantenere un \emph{sample}, per effettuare il \emph{sampling} della Stream esistono due approcci:
\begin{enumerate}
    \item Campionare gli elementi in modo proporzionale alla grandezza della Stream, come ad esempio mantenere $\frac{1}{10}$ degli elementi.
    \item Mantenere un campione casuale di una grandezza fissata su una Stream arbitrariamente grande. 
    Ad ogni passo $k$ si vuole un campione di $s$ elementi da mantenere in memoria, c'è da definire la proprietà degli elementi da mantenere. 
    
    \textbf{Dinamicità della Stream}: per ogni passo $k$ ogniuno dei $k$ elementi visto fino ad ora deve avere la stessa probabilità di essere nel campione.
    \vspace{1em}\noindent
    
\end{enumerate}
\subsection{Problemi di campionamento}
\subsubsection{Problema 1: Campione a porzione fissa}
Si assuma lo scenario in cui la Stream $U$ è composta da \emph{query} dei motori di ricerca, ovvero ogni elemento della Stream è una tupla \textit{(ID utente, query, tempo)}. Lo scopo principale è quello di trovare un campione $S \subseteq U$ che per, un utente medio $u$ e una query $q$, approssimi bene la frazione di occorrenze $q$ in $U$ fatte da $u$.
\begin{definition}[Occorrenza $q$]
    Una tupla in $U$ è chiamata \emph{occorrenza $q$} se ha il compomente $query = q$.
\end{definition}
